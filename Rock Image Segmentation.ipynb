{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a79fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage import measure\n",
    "from shapely.geometry import Polygon,Point\n",
    "from skimage import graph, data, io, segmentation, color\n",
    "from skimage.draw import polygon\n",
    "import copy\n",
    "from skimage.io import imsave, imread\n",
    "from skimage.measure import regionprops\n",
    "from skimage.segmentation import find_boundaries\n",
    "from networkx.linalg import adjacency_matrix\n",
    "from skimage.segmentation import slic,mark_boundaries\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(phi):  #Making a mask\n",
    "    N,M = phi.shape\n",
    "    re_PHI = np.zeros([N-1,M-1])\n",
    "    for i in range(N-1):\n",
    "        for j in range(M-1):\n",
    "            if phi[i,j] >= 0.0:\n",
    "                re_PHI[i,j] = 1\n",
    "            else:\n",
    "                re_PHI[i,j] = 0\n",
    "    return re_PHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df54fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeleteSmall(contours,num): # Delete small contours\n",
    "    count = 0\n",
    "    for i in range(len(contours)):\n",
    "        if(len(contours[i-count])<num):\n",
    "            contours.pop(i-count)\n",
    "            count += 1\n",
    "    return contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af6f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeX_Y(L): #Adjust the contour format\n",
    "    l = np.copy(L)\n",
    "    for i in range(len(L)):\n",
    "        l[i][0]=L[i][1]\n",
    "        l[i][1]=L[i][0]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f02b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Complete_Contour(contours,N,M): #Complete the contours located at the edges \n",
    "    contours_new = np.copy(contours)\n",
    "    for i in range(len(contours)):\n",
    "        cnt = contours[i]\n",
    "        cnt = np.array(cnt,dtype=np.int)\n",
    "        cnt = changeX_Y(cnt)\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if (x==0) & (y==0):\n",
    "            cnt = np.append(cnt, [[0,0]],axis=0)\n",
    "        if (x==0) & ((y+h)==N):\n",
    "            cnt = np.append(cnt, [[0,N-1]],axis=0)\n",
    "        if ((x+w)==M) & (y==0):\n",
    "            cnt = np.append(cnt, [[M-1,0]],axis=0)\n",
    "        if ((x+w)==M) & ((y+h)==N):\n",
    "            cnt = np.append(cnt, [[M-1,N-1]],axis=0)\n",
    "        contours_new[i] = cnt\n",
    "    return contours_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeleteIn(frontground,contours,im): #Remove small contour units within large contours\n",
    "    N,M,_= im.shape\n",
    "    rock_mask = np.ones((N,M),np.uint8)\n",
    "    for i in range(len(contours)):\n",
    "        cv2.fillPoly(rock_mask,[contours[i]],(0))\n",
    "    frontground = cv2.bitwise_and(im,im,mask=rock_mask)\n",
    "    return frontground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb98534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TagArea(contours,im): #Make Tag\n",
    "    N,M,_ = im.shape\n",
    "    tag_area = np.zeros_like(im,np.int32)\n",
    "    for i in range(len(contours)):\n",
    "        mask = np.zeros((N,M),np.int32)\n",
    "        cv2.fillPoly(mask,[contours[i]],(1))\n",
    "        tag_area[mask == 1] = i+1\n",
    "    return tag_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "fig1 = plt.figure(1)\n",
    "fig2 = plt.figure(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ece769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_fig2(phi, img): #Draw contours\n",
    "    plt.clf()\n",
    "    contours = measure.find_contours(phi, 0.5)\n",
    "    contours = DeleteSmall(contours,20)\n",
    "    fig2, ax2 = plt.subplots(figsize=(9, 9))\n",
    "    \n",
    "    ax2.imshow(img)\n",
    "    \n",
    "    for n, contour in enumerate(contours):\n",
    "        ax2.plot(contour[:, 1], contour[:, 0], linewidth=1.5, color='r')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e8f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(a, b, f): #Replace the parts that are less than 0\n",
    "    mask = f < 0\n",
    "    x = b.clone()\n",
    "    x[mask] = a[mask]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd78dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guassian_blur(img, kernel_size, sigma): #Gaussian filtering \n",
    "    device = img.device\n",
    "    kernel = torch.Tensor([[torch.exp(-0.5 * (i - kernel_size//2)**2 / sigma**2) *\n",
    "                                torch.exp(-0.5 * (j - kernel_size//2)**2 / sigma**2)\n",
    "                                for i in range(kernel_size)] for j in range(kernel_size)]).to(device)\n",
    "    kernel = kernel / kernel.sum()  \n",
    "    blurred = F.conv2d(img.unsqueeze(0).unsqueeze(0), kernel.unsqueeze(0).unsqueeze(0), padding=1)  \n",
    "    blur = blurred.squeeze(0).squeeze(0)\n",
    "    return blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b76cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_gradient_like_np(tensor): #Calculate the gradient of a tensor\n",
    "    if tensor.dim() != 2:\n",
    "        raise ValueError(\"Input tensor must be 2D.\")\n",
    "    # Calculate gradients using central differences\n",
    "    dy = tensor[1:, :] - tensor[:-1, :]\n",
    "    dx = tensor[:, 1:] - tensor[:, :-1]\n",
    "\n",
    "    # Pad gradients to match the original tensor shape\n",
    "    dy_padded = torch.cat((dy, torch.zeros(1, tensor.shape[1], device=tensor.device)), dim=0)\n",
    "    dx_padded = torch.cat((dx, torch.zeros(tensor.shape[0], 1, device=tensor.device)), dim=1)\n",
    "\n",
    "    return dy_padded, dx_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171775e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GLFIF(Img, LImg, u0, sigma, lambda1, lambda2, alpha1, alpha2, g):\n",
    "    u1 = u0**2\n",
    "    u2 = (1 - u0)**2\n",
    "\n",
    "    Iu1 = Img * u1\n",
    "    Iu2 = Img * u2\n",
    "\n",
    "    c1 = torch.sum(Iu1) / torch.sum(u1)\n",
    "    c2 = torch.sum(Iu2) / torch.sum(u2)\n",
    "\n",
    "\n",
    "    Ku1 = guassian_blur(u1, 3, sigma)\n",
    "    Ku2 = guassian_blur(u2, 3, sigma)\n",
    "\n",
    "    KI1 = guassian_blur(Iu1, 3, sigma)\n",
    "    KI2 = guassian_blur(Iu2, 3, sigma)\n",
    "\n",
    "    s1 = KI1 / Ku1\n",
    "    s2 = KI2 / Ku2\n",
    "\n",
    "    kim = (c1 * u1) + (c2 * u2)\n",
    "    DcH = (LImg - kim) * LImg\n",
    "    F3_old = DcH\n",
    "\n",
    "    sim = (s1 * u1) + (s2 * u2)\n",
    "    DsH = (LImg - sim) * LImg\n",
    "    F4_old = DsH\n",
    "\n",
    "    un = 1 / (1 + (lambda1 * (Img - c1)**2 + (alpha1 * s2 + alpha2 * c2)) / (\n",
    "            lambda2 * (Img - c2)**2 + (alpha1 * s1 + alpha2 * c1)))\n",
    "\n",
    "    un1 = un**2\n",
    "    un2 = (1 - un)**2\n",
    "\n",
    "    delta_u1 = un1 - u1\n",
    "    delta_u2 = un2 - u2\n",
    "\n",
    "    delta_F1 = un1 * delta_u1 * ((Img - c1)**2 / (un1 + delta_u1))\n",
    "    delta_F2 = un2 * delta_u2 * ((Img - c2)**2 / (un2 + delta_u2))\n",
    "\n",
    "    NIu1 = LImg * un1\n",
    "    NIu2 = LImg * un2\n",
    "\n",
    "    Nc1 = torch.sum(NIu1) / torch.sum(un1)\n",
    "    Nc2 = torch.sum(NIu2) / torch.sum(un2)\n",
    "\n",
    "    NK1 = NIu1 / un1\n",
    "    NK2 = NIu2 / un2\n",
    "\n",
    "    Nkim = un1 * Nc1 + un2 * Nc2\n",
    "    F3_new = (LImg - Nkim) * LImg\n",
    "\n",
    "    NKu1 = guassian_blur(un1, 3, sigma)\n",
    "    NKu2 = guassian_blur(un2, 3, sigma)\n",
    "\n",
    "    NKI1 = guassian_blur(NIu1, 3, sigma)\n",
    "    NKI2 = guassian_blur(NIu2, 3, sigma)\n",
    "\n",
    "    Ns1 = NKI1 / NKu1\n",
    "    Ns2 = NKI2 / NKu2\n",
    "\n",
    "    Nsim = un1 * Ns1 + un2 * Ns2\n",
    "    F4_new = (LImg - Nsim) * LImg\n",
    "\n",
    "    deltaF = lambda1 * delta_F1 * g + lambda2 * delta_F2 * g + alpha1 * (F3_new - F3_old) * g + alpha2 * (F4_new - F4_old) * g\n",
    "    # Add the function 'find' for updating 'u0' here\n",
    "    u = find(un, u0, deltaF)\n",
    "    # Perform Gaussian filtering on 'u' for smoothing\n",
    "    u = guassian_blur(u, 3, sigma)\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5e475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_lsf(Img, initial_lsf, iter_num, sigma, lambda1, lambda2, alpha1, alpha2): #Perform GLFIF\n",
    "\n",
    "    if Img.dim() != 2:\n",
    "        raise Exception(\"Please enter a grayscale image.\")\n",
    "\n",
    "    if Img.shape != initial_lsf.shape:\n",
    "        raise Exception(\"The input image size must match the initial_lsf.\")\n",
    "\n",
    "    if torch.max(Img) <= 1:\n",
    "        raise Exception(\"The grayscale values must be between 0 and 1!\")\n",
    "\n",
    "    img_smooth = guassian_blur(Img, 3, sigma) \n",
    "\n",
    "    \n",
    "    # Calculate the gradient\n",
    "    dy, dx = torch_gradient_like_np(img_smooth)\n",
    "    f = dy**2 + dx**2\n",
    "    g = 1 / (1 + f) \n",
    "    phi = initial_lsf.clone()\n",
    "\n",
    "    for n in range(iter_num):\n",
    "        phi = GLFIF(Img, Img, phi, sigma, lambda1, lambda2, alpha1, alpha2, g)\n",
    "    \n",
    "    return phi"
   ]
  },
  
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af31b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Level_Set(img, iter_num, sigma, lambda1, lambda2, alpha1, alpha2): #Params of GLFIF\n",
    "    device = img.device\n",
    "    N, M = img.shape\n",
    "    initial_lsf = torch.zeros(N, M, dtype=torch.float32).to(device)\n",
    "    initial_lsf[:, :] = 0.3\n",
    "    initial_lsf[0:1, 0:1] = 0.7\n",
    "\n",
    "    params = {\n",
    "        'Img': img,\n",
    "        'initial_lsf': initial_lsf,\n",
    "        'iter_num': iter_num,\n",
    "        'sigma': sigma,\n",
    "        'lambda1': lambda1,\n",
    "        'lambda2': lambda2,\n",
    "        'alpha1': alpha1,\n",
    "        'alpha2': alpha2\n",
    "    }\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dd9f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLS(img,output): #Calculate the segmentation result of Adaptive GLFIF\n",
    "    output = output.squeeze()\n",
    "    params = Level_Set(img,torch.tensor([[50]]),torch.tensor([[0.1]]),\n",
    "                           output[0].float(),output[1].float(),output[2].float(),output[3].float())\n",
    "    PHI = change_lsf(**params)\n",
    "\n",
    "    return PHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f0dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091b148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "input_channels = 1\n",
    "input_height = 256\n",
    "input_width = 256\n",
    "#Model Set\n",
    "num_layers = 2\n",
    "num_heads = 4\n",
    "hidden_dim = 2048\n",
    "dropout_rate = 0.1\n",
    "\n",
    "class AGAModel(nn.Module):  #The model returns Adaptive GLFIF results.\n",
    "    def __init__(self, input_channels, input_height, input_width, num_layers, num_heads, hidden_dim, dropout_rate):\n",
    "        super(AGAModel, self).__init__()\n",
    "        \n",
    "        resnet50 = models.resnet50(pretrained=False)\n",
    "        resnet50.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        resnet50.fc = nn.Identity()\n",
    "        self.feature_extractor = resnet50\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(hidden_dim, num_heads, dim_feedforward=hidden_dim, dropout=dropout_rate),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.parameter_generator = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 256),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 32), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 4), \n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        features = self.feature_extractor(x)\n",
    "        print(features.shape)\n",
    "        features = features.view(features.size(0), -1, features.size(1))\n",
    "        print(features.shape)\n",
    "        transformer_output = self.transformer_encoder(features)\n",
    "        print(transformer_output.shape)\n",
    "        parameters = self.parameter_generator(transformer_output) \n",
    "        print(parameters.shape)\n",
    "        print(parameters)\n",
    "        Im = x.squeeze()\n",
    "        phi = BLS(Im,parameters)\n",
    "        return phi\n",
    "\n",
    "net = AGAModel(input_channels, input_height, input_width, num_layers, num_heads, hidden_dim, dropout_rate).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ce3faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(\"./model/model_final.pth\")) #Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf4367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(contours, img): #show result\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    ax2.imshow(img)\n",
    "    \n",
    "    for n, contour in enumerate(contours):\n",
    "        ax2.plot(contour[:, 1], contour[:, 0], linewidth=1.2, color='r')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba8b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters, draw, measure, color\n",
    "from skimage.morphology import remove_small_objects\n",
    "from skimage import io, measure, morphology\n",
    "from scipy.ndimage import label\n",
    "from sklearn.decomposition import FastICA\n",
    "from skimage import filters\n",
    "def img_pre_ls(src,net): #Two-step segmentation algorithm\n",
    "    image = cv2.imread(src)\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    #Reduce the dimensionality of the image.\n",
    "    reshaped_img = img_lab.reshape(-1, 3)\n",
    "    pca = PCA(n_components=1)\n",
    "    im = pca.fit_transform(reshaped_img).reshape(img.shape[:2])\n",
    "    normalized_img = (im - np.min(im)) / (np.max(im) - np.min(im))\n",
    "    N,M = im.shape\n",
    "    standardized_img = (normalized_img * 255).astype(np.uint8)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.imshow(standardized_img, cmap='gray')\n",
    "    standardized_img = (normalized_img * 255).astype(np.uint8)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(standardized_img, cmap='gray')\n",
    "    plt.show()\n",
    "    standardized_img = torch.tensor(standardized_img).unsqueeze(0).unsqueeze(0).float()\n",
    "    standardized_img = standardized_img.to(device)\n",
    "    #Adaptive GLFIF Segmentation.\n",
    "    phi = net(standardized_img)\n",
    "    phi = phi.cpu().detach().numpy()\n",
    "    show_fig2(phi,img)\n",
    "    contours_low = measure.find_contours(phi, 0.5)\n",
    "    contours_low = DeleteSmall(contours_low,20)\n",
    "    phi_padded = np.pad(phi, pad_width=1, mode='constant', constant_values=0) #padded for get contours\n",
    "\n",
    "    contours = measure.find_contours(phi_padded, 0.5)\n",
    "    contours = DeleteSmall(contours,20)\n",
    "\n",
    "    print(len(contours))\n",
    "    tag = np.zeros_like(phi_padded, dtype=int)\n",
    "    for i, contour in enumerate(contours):\n",
    "        rr, cc = draw.polygon(contour[:, 0], contour[:, 1])\n",
    "        tag[rr, cc] = i + 1  \n",
    "    \n",
    "    T = np.copy(tag)\n",
    "    new_image = np.zeros((258, 258, 3), dtype=image.dtype)\n",
    "    new_image[1:-1, 1:-1, :] = image\n",
    "    \n",
    "    new_lab = np.zeros((258, 258, 3), dtype=img_lab.dtype)\n",
    "    new_lab[1:-1, 1:-1, :] = img_lab\n",
    "    \n",
    "    new_img = np.zeros((258, 258, 3), dtype=img.dtype)\n",
    "    new_img[1:-1, 1:-1, :] = img\n",
    "    Gray = new_img[:,:,0]\n",
    "\n",
    "    Gray[tag != 0] = 0\n",
    "    #Otsu menthod\n",
    "    threshold = filters.threshold_otsu(Gray)\n",
    "    binary_image = Gray > threshold\n",
    "    binary_image[tag != 0] = 0\n",
    "    plt.imshow(binary_image, cmap='gray')\n",
    "    plt.show()\n",
    "    max_label = np.max(tag)\n",
    "    binary_image_cleaned = remove_small_objects(binary_image, min_size=20)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(binary_image_cleaned, cmap='gray')\n",
    "    plt.show()\n",
    "    labeled_background, num_features =  morphology.label(binary_image_cleaned, return_num=True, connectivity=2)\n",
    "    labeled_background[labeled_background > 0] += max_label\n",
    "    tag[labeled_background != 0] = labeled_background[labeled_background != 0] #Final tag\n",
    "    print(np.unique(tag))\n",
    "    print(len(np.unique(tag)))\n",
    "    plt.imshow(tag, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    return tag,contours,phi_padded,T,binary_image_cleaned,contours_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecfa559",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"./test_image/image_001.png\" #test_image\n",
    "tag,contours,phi_padded,T,binary_image_cleaned,contours_low = img_pre_ls(src,net) #If an error occurs, please execute it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Show segmentation result.\n",
    "from skimage.segmentation import find_boundaries\n",
    "image = cv2.imread(src)\n",
    "image_lab = cv2.cvtColor(image, cv2.COLOR_BGR2Lab)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "flatten_lab = image_lab.reshape(-1, 3)\n",
    "pca = PCA(n_components=1)\n",
    "im = pca.fit_transform(flatten_lab).reshape(image.shape[:2])\n",
    "normalized_img = (im - np.min(im)) / (np.max(im) - np.min(im))\n",
    "N,M = im.shape\n",
    "standardized_img = (normalized_img * 255).astype(np.uint8)\n",
    "\n",
    "new_image = np.zeros((258, 258, 3), dtype=image.dtype)\n",
    "new_image[1:-1, 1:-1, :] = image\n",
    "boundaries = find_boundaries(tag,mode='thin')\n",
    "\n",
    "image_with_boundaries = np.copy(new_image)\n",
    "\n",
    "image_with_boundaries[boundaries] = [255, 0, 0]  \n",
    "plt.figure(figsize=(27,9))\n",
    "plt.subplot(1,3,1)\n",
    "plt.axis('off')\n",
    "plt.imshow(image, cmap='jet')\n",
    "plt.subplot(1,3,2)\n",
    "plt.axis('off')\n",
    "plt.imshow(new_image)\n",
    "for contour in contours:\n",
    "    plt.plot(contour[:, 1], contour[:, 0], linewidth=1.5, color='r')\n",
    "plt.subplot(1,3,3)\n",
    "plt.axis('off')\n",
    "plt.imshow(new_image)\n",
    "for label in np.unique(tag):\n",
    "    if label == 0:\n",
    "        continue  \n",
    "    contours_n = measure.find_contours(tag == label, level=0.5)\n",
    "    for contour in contours_n:\n",
    "        plt.plot(contour[:, 1], contour[:, 0], linewidth=1.5, color='r')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
